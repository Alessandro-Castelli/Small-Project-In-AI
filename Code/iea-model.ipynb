{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5216581,"sourceType":"datasetVersion","datasetId":3034772}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-24T15:27:10.069177Z","iopub.execute_input":"2024-02-24T15:27:10.069710Z","iopub.status.idle":"2024-02-24T15:27:10.987879Z","shell.execute_reply.started":"2024-02-24T15:27:10.069680Z","shell.execute_reply":"2024-02-24T15:27:10.987083Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/iea-monthly-electricity-statistics/data.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\nimport numpy as np\nimport math\nimport os\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport torch.nn as nn\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-02-24T15:27:10.989322Z","iopub.execute_input":"2024-02-24T15:27:10.989693Z","iopub.status.idle":"2024-02-24T15:27:14.977409Z","shell.execute_reply.started":"2024-02-24T15:27:10.989667Z","shell.execute_reply":"2024-02-24T15:27:14.976567Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/iea-monthly-electricity-statistics/data.csv')\nprint(df)","metadata":{"execution":{"iopub.status.busy":"2024-02-24T15:27:14.981231Z","iopub.execute_input":"2024-02-24T15:27:14.981592Z","iopub.status.idle":"2024-02-24T15:27:15.573499Z","shell.execute_reply.started":"2024-02-24T15:27:14.981568Z","shell.execute_reply":"2024-02-24T15:27:15.572404Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"              COUNTRY CODE_TIME           TIME  YEAR  MONTH MONTH_NAME  \\\n0           Australia   JAN2010   January 2010  2010      1    January   \n1           Australia   JAN2010   January 2010  2010      1    January   \n2           Australia   JAN2010   January 2010  2010      1    January   \n3           Australia   JAN2010   January 2010  2010      1    January   \n4           Australia   JAN2010   January 2010  2010      1    January   \n...               ...       ...            ...   ...    ...        ...   \n181910  United States   DEC2022  December 2022  2022     12   December   \n181911  United States   DEC2022  December 2022  2022     12   December   \n181912  United States   DEC2022  December 2022  2022     12   December   \n181913  United States   DEC2022  December 2022  2022     12   December   \n181914  United States   DEC2022  December 2022  2022     12   December   \n\n                            PRODUCT          VALUE  DISPLAY_ORDER  \\\n0                             Hydro     990.728000              1   \n1                              Wind     409.469000              2   \n2                             Solar      49.216000              3   \n3                        Geothermal       0.083000              4   \n4           Total combustible fuels   19289.730000              7   \n...                             ...            ...            ...   \n181910               Non-renewables  292417.548132             23   \n181911                       Others    8017.840957             24   \n181912  Other renewables aggregated    6133.265943             25   \n181913                   Low carbon  146425.474534             26   \n181914                 Fossil fuels  223357.219650             27   \n\n          yearToDate  previousYearToDate     share  \n0       1.647189e+04                 NaN  0.047771  \n1       4.940909e+03                 NaN  0.019744  \n2       9.082380e+02                 NaN  0.002373  \n3       9.960000e-01                 NaN  0.000004  \n4       2.143030e+05                 NaN  0.930108  \n...              ...                 ...       ...  \n181910  3.355042e+06        3.320634e+06  0.791164  \n181911  5.393606e+04        4.899452e+04  0.021693  \n181912  7.100997e+04        7.242158e+04  0.016594  \n181913  1.749805e+06        1.670531e+06  0.396168  \n181914  2.583925e+06        2.542138e+06  0.604315  \n\n[181915 rows x 12 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"df = df[df['PRODUCT'] == 'Solar']\nprint(df)","metadata":{"execution":{"iopub.status.busy":"2024-02-24T15:27:15.574791Z","iopub.execute_input":"2024-02-24T15:27:15.575284Z","iopub.status.idle":"2024-02-24T15:27:15.605085Z","shell.execute_reply.started":"2024-02-24T15:27:15.575260Z","shell.execute_reply":"2024-02-24T15:27:15.604279Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"               COUNTRY CODE_TIME           TIME  YEAR  MONTH MONTH_NAME  \\\n2            Australia   JAN2010   January 2010  2010      1    January   \n22             Austria   JAN2010   January 2010  2010      1    January   \n47             Belgium   JAN2010   January 2010  2010      1    January   \n72              Canada   JAN2010   January 2010  2010      1    January   \n118     Czech Republic   JAN2010   January 2010  2010      1    January   \n...                ...       ...            ...   ...    ...        ...   \n181793           Spain   DEC2022  December 2022  2022     12   December   \n181819          Sweden   DEC2022  December 2022  2022     12   December   \n181843     Switzerland   DEC2022  December 2022  2022     12   December   \n181866  United Kingdom   DEC2022  December 2022  2022     12   December   \n181891   United States   DEC2022  December 2022  2022     12   December   \n\n       PRODUCT        VALUE  DISPLAY_ORDER     yearToDate  previousYearToDate  \\\n2        Solar    49.216000              3     908.238000                 NaN   \n22       Solar     2.864000              3      84.421000                 NaN   \n47       Solar    17.450000              3     560.000000                 NaN   \n72       Solar     1.898000              3     253.000000                 NaN   \n118      Solar     6.205000              3     612.002000                 NaN   \n...        ...          ...            ...            ...                 ...   \n181793   Solar  1205.878240              3   32816.269966        25937.966502   \n181819   Solar     6.469757              3    1526.000000         1051.000000   \n181843   Solar    48.943794              3    3794.775534         2891.866818   \n181866   Solar   508.798542              3   14240.791059        12550.630235   \n181891   Solar  9172.684879              3  188300.466584       152294.007362   \n\n           share  \n2       0.002373  \n22      0.000485  \n47      0.002092  \n72      0.000031  \n118     0.000799  \n...          ...  \n181793  0.053158  \n181819  0.000414  \n181843  0.009261  \n181866  0.018057  \n181891  0.024818  \n\n[7054 rows x 12 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"df = df.drop('PRODUCT', axis=1)\ndf = df.drop('MONTH_NAME', axis=1)\ndf = df.drop('TIME', axis=1)\ndf = df.drop('CODE_TIME', axis=1)\ndf = df.drop('DISPLAY_ORDER', axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-02-24T15:27:15.606194Z","iopub.execute_input":"2024-02-24T15:27:15.606516Z","iopub.status.idle":"2024-02-24T15:27:15.622852Z","shell.execute_reply.started":"2024-02-24T15:27:15.606486Z","shell.execute_reply":"2024-02-24T15:27:15.621525Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"country_encoder = LabelEncoder()\ndf['COUNTRY'] = country_encoder.fit_transform(df['COUNTRY'])","metadata":{"execution":{"iopub.status.busy":"2024-02-24T15:27:15.623906Z","iopub.execute_input":"2024-02-24T15:27:15.624196Z","iopub.status.idle":"2024-02-24T15:27:15.631574Z","shell.execute_reply.started":"2024-02-24T15:27:15.624172Z","shell.execute_reply":"2024-02-24T15:27:15.630504Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(df)","metadata":{"execution":{"iopub.status.busy":"2024-02-24T15:27:15.632805Z","iopub.execute_input":"2024-02-24T15:27:15.633307Z","iopub.status.idle":"2024-02-24T15:27:15.647872Z","shell.execute_reply.started":"2024-02-24T15:27:15.633277Z","shell.execute_reply":"2024-02-24T15:27:15.646817Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"        COUNTRY  YEAR  MONTH        VALUE     yearToDate  previousYearToDate  \\\n2             1  2010      1    49.216000     908.238000                 NaN   \n22            2  2010      1     2.864000      84.421000                 NaN   \n47            3  2010      1    17.450000     560.000000                 NaN   \n72            6  2010      1     1.898000     253.000000                 NaN   \n118          12  2010      1     6.205000     612.002000                 NaN   \n...         ...   ...    ...          ...            ...                 ...   \n181793       47  2022     12  1205.878240   32816.269966        25937.966502   \n181819       48  2022     12     6.469757    1526.000000         1051.000000   \n181843       49  2022     12    48.943794    3794.775534         2891.866818   \n181866       50  2022     12   508.798542   14240.791059        12550.630235   \n181891       51  2022     12  9172.684879  188300.466584       152294.007362   \n\n           share  \n2       0.002373  \n22      0.000485  \n47      0.002092  \n72      0.000031  \n118     0.000799  \n...          ...  \n181793  0.053158  \n181819  0.000414  \n181843  0.009261  \n181866  0.018057  \n181891  0.024818  \n\n[7054 rows x 7 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"df = df.dropna() # Removes rows with null values\ndf.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-24T15:27:15.650158Z","iopub.execute_input":"2024-02-24T15:27:15.650543Z","iopub.status.idle":"2024-02-24T15:27:15.660833Z","shell.execute_reply.started":"2024-02-24T15:27:15.650517Z","shell.execute_reply":"2024-02-24T15:27:15.659297Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"print(df)\nprint(df.info())","metadata":{"execution":{"iopub.status.busy":"2024-02-24T15:27:15.663622Z","iopub.execute_input":"2024-02-24T15:27:15.663969Z","iopub.status.idle":"2024-02-24T15:27:15.803717Z","shell.execute_reply.started":"2024-02-24T15:27:15.663945Z","shell.execute_reply":"2024-02-24T15:27:15.802689Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"      COUNTRY  YEAR  MONTH        VALUE     yearToDate  previousYearToDate  \\\n0           1  2011      1   176.116000    1875.002000           49.216000   \n1           2  2011      1     5.607000     165.283000            2.864000   \n2           3  2011      1    19.659000    1169.000000           17.450000   \n3           6  2011      1     7.373000     565.998000            1.898000   \n4          12  2011      1    35.765000    2169.001000            6.205000   \n...       ...   ...    ...          ...            ...                 ...   \n6413       47  2022     12  1205.878240   32816.269966        25937.966502   \n6414       48  2022     12     6.469757    1526.000000         1051.000000   \n6415       49  2022     12    48.943794    3794.775534         2891.866818   \n6416       50  2022     12   508.798542   14240.791059        12550.630235   \n6417       51  2022     12  9172.684879  188300.466584       152294.007362   \n\n         share  \n0     0.007500  \n1     0.000914  \n2     0.002546  \n3     0.000119  \n4     0.004673  \n...        ...  \n6413  0.053158  \n6414  0.000414  \n6415  0.009261  \n6416  0.018057  \n6417  0.024818  \n\n[6418 rows x 7 columns]\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 6418 entries, 0 to 6417\nData columns (total 7 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   COUNTRY             6418 non-null   int64  \n 1   YEAR                6418 non-null   int64  \n 2   MONTH               6418 non-null   int64  \n 3   VALUE               6418 non-null   float64\n 4   yearToDate          6418 non-null   float64\n 5   previousYearToDate  6418 non-null   float64\n 6   share               6418 non-null   float64\ndtypes: float64(4), int64(3)\nmemory usage: 351.1 KB\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"# Select columns to normalize\ncolumns_to_normalize = ['VALUE', 'yearToDate', 'previousYearToDate', 'share']\n\n# Create a scaler\nscaler = StandardScaler()\n\n# Normalize the selected columns\ndf[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])","metadata":{"execution":{"iopub.status.busy":"2024-02-24T15:27:15.804936Z","iopub.execute_input":"2024-02-24T15:27:15.805291Z","iopub.status.idle":"2024-02-24T15:27:15.816200Z","shell.execute_reply.started":"2024-02-24T15:27:15.805254Z","shell.execute_reply":"2024-02-24T15:27:15.815222Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print(df)","metadata":{"execution":{"iopub.status.busy":"2024-02-24T15:27:15.817145Z","iopub.execute_input":"2024-02-24T15:27:15.817578Z","iopub.status.idle":"2024-02-24T15:27:15.828996Z","shell.execute_reply.started":"2024-02-24T15:27:15.817557Z","shell.execute_reply":"2024-02-24T15:27:15.827950Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"      COUNTRY  YEAR  MONTH     VALUE  yearToDate  previousYearToDate     share\n0           1  2011      1 -0.311911   -0.330786           -0.293959 -0.582787\n1           2  2011      1 -0.339521   -0.355004           -0.295184 -0.764550\n2           3  2011      1 -0.337245   -0.340787           -0.294798 -0.719497\n3           6  2011      1 -0.339235   -0.349328           -0.295209 -0.786489\n4          12  2011      1 -0.334637   -0.326622           -0.295095 -0.660808\n...       ...   ...    ...       ...         ...                 ...       ...\n6413       47  2022     12 -0.145164    0.107482            0.390069  0.677143\n6414       48  2022     12 -0.339381   -0.335730           -0.267490 -0.778344\n6415       49  2022     12 -0.332503   -0.303594           -0.218851 -0.534190\n6416       50  2022     12 -0.258040   -0.155631            0.036351 -0.291481\n6417       51  2022     12  1.144878    2.309839            3.728625 -0.104912\n\n[6418 rows x 7 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Apply one-hot encoding to the 'COUNTRY' column\ndf = pd.get_dummies(df, columns=['COUNTRY'], prefix='country')\ndf = pd.get_dummies(df, columns=['MONTH'], prefix='month')\ndf = pd.get_dummies(df, columns=['YEAR'], prefix='year')\n\n\nboolean_columns = [\n    'country_0', 'country_1', 'country_2', 'country_3', 'country_4', 'country_5',\n    'country_6', 'country_7', 'country_8', 'country_9', 'country_10', 'country_11',\n    'country_12', 'country_13', 'country_14', 'country_15', 'country_16', 'country_17',\n    'country_18', 'country_19', 'country_20', 'country_21', 'country_22', 'country_23',\n    'country_24', 'country_25', 'country_26', 'country_27', 'country_28', 'country_29',\n    'country_30', 'country_31', 'country_32', 'country_33', 'country_34', 'country_35',\n    'country_36', 'country_37', 'country_38', 'country_39', 'country_40', 'country_41',\n    'country_42', 'country_43', 'country_44', 'country_45', 'country_46', 'country_47',\n    'country_48', 'country_49', 'country_50', 'country_51',\n    'year_2011', 'year_2012', 'year_2013', 'year_2014', 'year_2015', 'year_2016',\n    'year_2017', 'year_2018', 'year_2019', 'year_2020','year_2021','year_2022',\n    'month_1', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12'\n   \n]\n\ndf[boolean_columns] = df[boolean_columns].astype(int)\n\n# Print the resulting dataframe\nprint(df)","metadata":{"execution":{"iopub.status.busy":"2024-02-24T15:27:15.830339Z","iopub.execute_input":"2024-02-24T15:27:15.830644Z","iopub.status.idle":"2024-02-24T15:27:15.869095Z","shell.execute_reply.started":"2024-02-24T15:27:15.830624Z","shell.execute_reply":"2024-02-24T15:27:15.867861Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"         VALUE  yearToDate  previousYearToDate     share  country_0  \\\n0    -0.311911   -0.330786           -0.293959 -0.582787          0   \n1    -0.339521   -0.355004           -0.295184 -0.764550          0   \n2    -0.337245   -0.340787           -0.294798 -0.719497          0   \n3    -0.339235   -0.349328           -0.295209 -0.786489          0   \n4    -0.334637   -0.326622           -0.295095 -0.660808          0   \n...        ...         ...                 ...       ...        ...   \n6413 -0.145164    0.107482            0.390069  0.677143          0   \n6414 -0.339381   -0.335730           -0.267490 -0.778344          0   \n6415 -0.332503   -0.303594           -0.218851 -0.534190          0   \n6416 -0.258040   -0.155631            0.036351 -0.291481          0   \n6417  1.144878    2.309839            3.728625 -0.104912          0   \n\n      country_1  country_2  country_3  country_4  country_5  ...  year_2013  \\\n0             1          0          0          0          0  ...          0   \n1             0          1          0          0          0  ...          0   \n2             0          0          1          0          0  ...          0   \n3             0          0          0          0          0  ...          0   \n4             0          0          0          0          0  ...          0   \n...         ...        ...        ...        ...        ...  ...        ...   \n6413          0          0          0          0          0  ...          0   \n6414          0          0          0          0          0  ...          0   \n6415          0          0          0          0          0  ...          0   \n6416          0          0          0          0          0  ...          0   \n6417          0          0          0          0          0  ...          0   \n\n      year_2014  year_2015  year_2016  year_2017  year_2018  year_2019  \\\n0             0          0          0          0          0          0   \n1             0          0          0          0          0          0   \n2             0          0          0          0          0          0   \n3             0          0          0          0          0          0   \n4             0          0          0          0          0          0   \n...         ...        ...        ...        ...        ...        ...   \n6413          0          0          0          0          0          0   \n6414          0          0          0          0          0          0   \n6415          0          0          0          0          0          0   \n6416          0          0          0          0          0          0   \n6417          0          0          0          0          0          0   \n\n      year_2020  year_2021  year_2022  \n0             0          0          0  \n1             0          0          0  \n2             0          0          0  \n3             0          0          0  \n4             0          0          0  \n...         ...        ...        ...  \n6413          0          0          1  \n6414          0          0          1  \n6415          0          0          1  \n6416          0          0          1  \n6417          0          0          1  \n\n[6418 rows x 80 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Model definition\nclass LinearRegressionModel(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LinearRegressionModel, self).__init__()\n        self.linear = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        out = self.linear(x)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-02-24T15:27:15.870441Z","iopub.execute_input":"2024-02-24T15:27:15.870705Z","iopub.status.idle":"2024-02-24T15:27:15.876247Z","shell.execute_reply.started":"2024-02-24T15:27:15.870683Z","shell.execute_reply":"2024-02-24T15:27:15.875089Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class RegressionModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(RegressionModel, self).__init__()\n        self.layer1 = nn.Linear(input_dim, hidden_dim)\n        self.relu = nn.ReLU()\n        self.layer2 = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.relu(out)\n        out = self.layer2(out)\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2024-02-24T15:27:15.877483Z","iopub.execute_input":"2024-02-24T15:27:15.877836Z","iopub.status.idle":"2024-02-24T15:27:15.887504Z","shell.execute_reply.started":"2024-02-24T15:27:15.877812Z","shell.execute_reply":"2024-02-24T15:27:15.886589Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class RNNRegressionModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(RNNRegressionModel, self).__init__()\n        self.rnn = nn.RNN(input_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        out, _ = self.rnn(x)\n        if len(out.shape) > 2:\n            out = self.fc(out[:, -1, :])\n        else:\n            out = self.fc(out)\n        return out.squeeze(1) if len(out.shape) > 2 else out","metadata":{"execution":{"iopub.status.busy":"2024-02-24T15:27:15.888554Z","iopub.execute_input":"2024-02-24T15:27:15.888787Z","iopub.status.idle":"2024-02-24T15:27:15.901376Z","shell.execute_reply.started":"2024-02-24T15:27:15.888766Z","shell.execute_reply":"2024-02-24T15:27:15.900577Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"target_index = df.columns.get_loc('VALUE')\n\nX = df.iloc[:, target_index+1:]\n\ny = df['VALUE']\n\n# Split the dataset into training, validation, and test sets\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Number of input and output features\ninput_dim = X_train.shape[1]\noutput_dim = 1\n\n# Initialize the model\n#model = LinearRegressionModel(input_dim, output_dim)\n\n# # Default Initialization\n# nn.init.xavier_uniform_(model.linear.weight)\n# nn.init.zeros_(model.linear.bias)\n\n#model = RegressionModel(input_dim, input_dim*2, output_dim)\nprint(input_dim)\nmodel = LinearRegressionModel(input_dim, output_dim)\nmodel = RegressionModel(input_dim, input_dim*2, output_dim)\nmodel = RNNRegressionModel(input_dim, input_dim*2, output_dim)\n\n\n# Define the loss function and optimizer\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.00001)\n\nprint(df.info())","metadata":{"execution":{"iopub.status.busy":"2024-02-24T15:27:15.902716Z","iopub.execute_input":"2024-02-24T15:27:15.903284Z","iopub.status.idle":"2024-02-24T15:27:18.182748Z","shell.execute_reply.started":"2024-02-24T15:27:15.903256Z","shell.execute_reply":"2024-02-24T15:27:18.181854Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"79\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 6418 entries, 0 to 6417\nData columns (total 80 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   VALUE               6418 non-null   float64\n 1   yearToDate          6418 non-null   float64\n 2   previousYearToDate  6418 non-null   float64\n 3   share               6418 non-null   float64\n 4   country_0           6418 non-null   int64  \n 5   country_1           6418 non-null   int64  \n 6   country_2           6418 non-null   int64  \n 7   country_3           6418 non-null   int64  \n 8   country_4           6418 non-null   int64  \n 9   country_5           6418 non-null   int64  \n 10  country_6           6418 non-null   int64  \n 11  country_7           6418 non-null   int64  \n 12  country_8           6418 non-null   int64  \n 13  country_9           6418 non-null   int64  \n 14  country_10          6418 non-null   int64  \n 15  country_11          6418 non-null   int64  \n 16  country_12          6418 non-null   int64  \n 17  country_13          6418 non-null   int64  \n 18  country_14          6418 non-null   int64  \n 19  country_15          6418 non-null   int64  \n 20  country_16          6418 non-null   int64  \n 21  country_17          6418 non-null   int64  \n 22  country_18          6418 non-null   int64  \n 23  country_19          6418 non-null   int64  \n 24  country_20          6418 non-null   int64  \n 25  country_21          6418 non-null   int64  \n 26  country_22          6418 non-null   int64  \n 27  country_23          6418 non-null   int64  \n 28  country_24          6418 non-null   int64  \n 29  country_25          6418 non-null   int64  \n 30  country_26          6418 non-null   int64  \n 31  country_27          6418 non-null   int64  \n 32  country_28          6418 non-null   int64  \n 33  country_29          6418 non-null   int64  \n 34  country_30          6418 non-null   int64  \n 35  country_31          6418 non-null   int64  \n 36  country_32          6418 non-null   int64  \n 37  country_33          6418 non-null   int64  \n 38  country_34          6418 non-null   int64  \n 39  country_35          6418 non-null   int64  \n 40  country_36          6418 non-null   int64  \n 41  country_37          6418 non-null   int64  \n 42  country_38          6418 non-null   int64  \n 43  country_39          6418 non-null   int64  \n 44  country_40          6418 non-null   int64  \n 45  country_41          6418 non-null   int64  \n 46  country_42          6418 non-null   int64  \n 47  country_43          6418 non-null   int64  \n 48  country_44          6418 non-null   int64  \n 49  country_45          6418 non-null   int64  \n 50  country_46          6418 non-null   int64  \n 51  country_47          6418 non-null   int64  \n 52  country_48          6418 non-null   int64  \n 53  country_49          6418 non-null   int64  \n 54  country_50          6418 non-null   int64  \n 55  country_51          6418 non-null   int64  \n 56  month_1             6418 non-null   int64  \n 57  month_2             6418 non-null   int64  \n 58  month_3             6418 non-null   int64  \n 59  month_4             6418 non-null   int64  \n 60  month_5             6418 non-null   int64  \n 61  month_6             6418 non-null   int64  \n 62  month_7             6418 non-null   int64  \n 63  month_8             6418 non-null   int64  \n 64  month_9             6418 non-null   int64  \n 65  month_10            6418 non-null   int64  \n 66  month_11            6418 non-null   int64  \n 67  month_12            6418 non-null   int64  \n 68  year_2011           6418 non-null   int64  \n 69  year_2012           6418 non-null   int64  \n 70  year_2013           6418 non-null   int64  \n 71  year_2014           6418 non-null   int64  \n 72  year_2015           6418 non-null   int64  \n 73  year_2016           6418 non-null   int64  \n 74  year_2017           6418 non-null   int64  \n 75  year_2018           6418 non-null   int64  \n 76  year_2019           6418 non-null   int64  \n 77  year_2020           6418 non-null   int64  \n 78  year_2021           6418 non-null   int64  \n 79  year_2022           6418 non-null   int64  \ndtypes: float64(4), int64(76)\nmemory usage: 3.9 MB\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"import wandb\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"Label\")\n\nwandb.login(key=secret_value_0)\n\nwandb.init(project='IEA Model', save_code=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-24T15:27:18.183876Z","iopub.execute_input":"2024-02-24T15:27:18.184244Z","iopub.status.idle":"2024-02-24T15:27:52.935596Z","shell.execute_reply.started":"2024-02-24T15:27:18.184222Z","shell.execute_reply":"2024-02-24T15:27:52.934281Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33males-2000-09\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240224_152721-q8jpqwbg</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ales-2000-09/IEA%20Model/runs/q8jpqwbg' target=\"_blank\">trim-paper-6</a></strong> to <a href='https://wandb.ai/ales-2000-09/IEA%20Model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ales-2000-09/IEA%20Model' target=\"_blank\">https://wandb.ai/ales-2000-09/IEA%20Model</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ales-2000-09/IEA%20Model/runs/q8jpqwbg' target=\"_blank\">https://wandb.ai/ales-2000-09/IEA%20Model/runs/q8jpqwbg</a>"},"metadata":{}},{"execution_count":17,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ales-2000-09/IEA%20Model/runs/q8jpqwbg?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7f0e2e613580>"},"metadata":{}}]},{"cell_type":"code","source":"# Training\nepochs = 128\nbatch_size = 1\nwandb.log({'Epoch': epochs})\nwandb.log({'batch_size': batch_size})\n\nfor epoch in range(epochs):\n    model.train()\n    for i in range(0, X_train.shape[0], batch_size):\n        inputs = torch.from_numpy(X_train.iloc[i:i+batch_size].values).float()\n        labels = torch.from_numpy(y_train.iloc[i:i+batch_size].values).float().unsqueeze(1)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n    # Validation\n    model.eval()\n    val_loss_total = 0.0\n\n    with torch.no_grad():\n        for i in range(0, X_val.shape[0], batch_size):\n            val_batch_inputs = torch.from_numpy(X_val.iloc[i:i+batch_size].values).float()\n            val_batch_labels = torch.from_numpy(y_val.iloc[i:i+batch_size].values).float().unsqueeze(1)\n\n            val_outputs = model(val_batch_inputs)\n            val_loss = criterion(val_outputs, val_batch_labels)\n            val_loss_total += val_loss.item()\n\n    avg_val_loss = val_loss_total / (X_val.shape[0] / batch_size)\n\n    print(f'Epoch {epoch+1}/{epochs}, Training Loss: {loss.item()}, Validation Loss: {avg_val_loss}')\n    wandb.log({'Training Loss': loss.item(), 'Validation Loss': avg_val_loss})\n\n# Test\nmodel.eval()\nwith torch.no_grad():\n    test_inputs = torch.from_numpy(X_test.values).float()\n    test_labels = torch.from_numpy(y_test.values).float().unsqueeze(1)\n    test_outputs = model(test_inputs)\n    test_loss = criterion(test_outputs, test_labels)\n\nprint(f'Test Loss: {test_loss.item()}')\nwandb.log({'Test Loss': test_loss.item()})\n\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-02-24T15:27:52.937176Z","iopub.execute_input":"2024-02-24T15:27:52.937483Z","iopub.status.idle":"2024-02-24T15:41:23.780537Z","shell.execute_reply.started":"2024-02-24T15:27:52.937455Z","shell.execute_reply":"2024-02-24T15:41:23.779578Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Epoch 1/128, Training Loss: 0.004873207770287991, Validation Loss: 1.2283970535166007\nEpoch 2/128, Training Loss: 0.003793954150751233, Validation Loss: 1.0512887768880792\nEpoch 3/128, Training Loss: 0.002449800493195653, Validation Loss: 0.9017127130750328\nEpoch 4/128, Training Loss: 0.0012067186180502176, Validation Loss: 0.7741150824461103\nEpoch 5/128, Training Loss: 0.0003385045565664768, Validation Loss: 0.6653483294028555\nEpoch 6/128, Training Loss: 2.0176448742859066e-06, Validation Loss: 0.5733791948369941\nEpoch 7/128, Training Loss: 0.0002371193841099739, Validation Loss: 0.496569376927838\nEpoch 8/128, Training Loss: 0.0009875455871224403, Validation Loss: 0.4333271934959351\nEpoch 9/128, Training Loss: 0.0021343855187296867, Validation Loss: 0.38199710638014384\nEpoch 10/128, Training Loss: 0.0035318685695528984, Validation Loss: 0.34087910347126693\nEpoch 11/128, Training Loss: 0.005036947783082724, Validation Loss: 0.30830570530499035\nEpoch 12/128, Training Loss: 0.006529178936034441, Validation Loss: 0.28272119219913755\nEpoch 13/128, Training Loss: 0.007919973693788052, Validation Loss: 0.2627390007413408\nEpoch 14/128, Training Loss: 0.009153162129223347, Validation Loss: 0.24717306926322935\nEpoch 15/128, Training Loss: 0.010200749151408672, Validation Loss: 0.23504197890364995\nEpoch 16/128, Training Loss: 0.011057001538574696, Validation Loss: 0.22555500090516575\nEpoch 17/128, Training Loss: 0.011730420403182507, Validation Loss: 0.21808911964630445\nEpoch 18/128, Training Loss: 0.012238997966051102, Validation Loss: 0.21216151013668877\nEpoch 19/128, Training Loss: 0.012605000287294388, Validation Loss: 0.20740257282221916\nEpoch 20/128, Training Loss: 0.012851216830313206, Validation Loss: 0.20353224514978846\nEpoch 21/128, Training Loss: 0.013000041246414185, Validation Loss: 0.20033927966693113\nEpoch 22/128, Training Loss: 0.013071831315755844, Validation Loss: 0.19766475365654684\nEpoch 23/128, Training Loss: 0.01308386866003275, Validation Loss: 0.19538937510667592\nEpoch 24/128, Training Loss: 0.013050945475697517, Validation Loss: 0.19342338531710976\nEpoch 25/128, Training Loss: 0.012985060922801495, Validation Loss: 0.19169915773373752\nEpoch 26/128, Training Loss: 0.012896141968667507, Validation Loss: 0.1901654604306138\nEpoch 27/128, Training Loss: 0.012792039662599564, Validation Loss: 0.18878301247708756\nEpoch 28/128, Training Loss: 0.012678766623139381, Validation Loss: 0.18752196847261865\nEpoch 29/128, Training Loss: 0.01256117969751358, Validation Loss: 0.18635901328951018\nEpoch 30/128, Training Loss: 0.01244249939918518, Validation Loss: 0.18527635145162852\nEpoch 31/128, Training Loss: 0.012325387448072433, Validation Loss: 0.18425998657903356\nEpoch 32/128, Training Loss: 0.01221176702529192, Validation Loss: 0.18329871749043217\nEpoch 33/128, Training Loss: 0.012102769687771797, Validation Loss: 0.1823837586793013\nEpoch 34/128, Training Loss: 0.011999537236988544, Validation Loss: 0.1815081714860553\nEpoch 35/128, Training Loss: 0.011902357451617718, Validation Loss: 0.18066628625656392\nEpoch 36/128, Training Loss: 0.0118114547803998, Validation Loss: 0.17985366432705926\nEpoch 37/128, Training Loss: 0.011727146804332733, Validation Loss: 0.179066517803875\nEpoch 38/128, Training Loss: 0.011649188585579395, Validation Loss: 0.17830192931570507\nEpoch 39/128, Training Loss: 0.011577523313462734, Validation Loss: 0.17755741470906816\nEpoch 40/128, Training Loss: 0.011511809192597866, Validation Loss: 0.17683094449834294\nEpoch 41/128, Training Loss: 0.011451983824372292, Validation Loss: 0.17612089120184182\nEpoch 42/128, Training Loss: 0.011397582478821278, Validation Loss: 0.17542579477903872\nEpoch 43/128, Training Loss: 0.011348350904881954, Validation Loss: 0.17474453591331016\nEpoch 44/128, Training Loss: 0.011303972452878952, Validation Loss: 0.17407621516414615\nEpoch 45/128, Training Loss: 0.011264191009104252, Validation Loss: 0.17341987370682782\nEpoch 46/128, Training Loss: 0.011228806339204311, Validation Loss: 0.17277482517106818\nEpoch 47/128, Training Loss: 0.011197185143828392, Validation Loss: 0.1721405421852006\nEpoch 48/128, Training Loss: 0.011169337667524815, Validation Loss: 0.17151656850940533\nEpoch 49/128, Training Loss: 0.01114494726061821, Validation Loss: 0.1709024386110428\nEpoch 50/128, Training Loss: 0.011123795993626118, Validation Loss: 0.17029780526641483\nEpoch 51/128, Training Loss: 0.011105578392744064, Validation Loss: 0.16970221914680558\nEpoch 52/128, Training Loss: 0.011090056970715523, Validation Loss: 0.16911557563025223\nEpoch 53/128, Training Loss: 0.011077167466282845, Validation Loss: 0.16853749503521098\nEpoch 54/128, Training Loss: 0.01106660533696413, Validation Loss: 0.16796777836961246\nEpoch 55/128, Training Loss: 0.011058380827307701, Validation Loss: 0.16740612121911216\nEpoch 56/128, Training Loss: 0.011052371002733707, Validation Loss: 0.1668524336378373\nEpoch 57/128, Training Loss: 0.011048046872019768, Validation Loss: 0.1663064292803249\nEpoch 58/128, Training Loss: 0.011045591905713081, Validation Loss: 0.1657680181098261\nEpoch 59/128, Training Loss: 0.0110448207706213, Validation Loss: 0.1652370305100141\nEpoch 60/128, Training Loss: 0.011045753955841064, Validation Loss: 0.16471338729290091\nEpoch 61/128, Training Loss: 0.011048278771340847, Validation Loss: 0.16419690565965134\nEpoch 62/128, Training Loss: 0.011052139103412628, Validation Loss: 0.1636874247939152\nEpoch 63/128, Training Loss: 0.011057402938604355, Validation Loss: 0.16318487715889501\nEpoch 64/128, Training Loss: 0.011063828133046627, Validation Loss: 0.16268922364311028\nEpoch 65/128, Training Loss: 0.011071472428739071, Validation Loss: 0.16220021487744218\nEpoch 66/128, Training Loss: 0.011080235242843628, Validation Loss: 0.16171774037781592\nEpoch 67/128, Training Loss: 0.011090037412941456, Validation Loss: 0.16124172268788015\nEpoch 68/128, Training Loss: 0.011100899428129196, Validation Loss: 0.16077209164229753\nEpoch 69/128, Training Loss: 0.011112765409052372, Validation Loss: 0.16030874700068556\nEpoch 70/128, Training Loss: 0.01112551148980856, Validation Loss: 0.15985155735845114\nEpoch 71/128, Training Loss: 0.01113898865878582, Validation Loss: 0.15940050155454244\nEpoch 72/128, Training Loss: 0.011153430677950382, Validation Loss: 0.15895546369412658\nEpoch 73/128, Training Loss: 0.011168682016432285, Validation Loss: 0.15851633049860042\nEpoch 74/128, Training Loss: 0.011184687726199627, Validation Loss: 0.15808293339966656\nEpoch 75/128, Training Loss: 0.011201499029994011, Validation Loss: 0.1576553014947925\nEpoch 76/128, Training Loss: 0.011218986473977566, Validation Loss: 0.15723325604236799\nEpoch 77/128, Training Loss: 0.01123721431940794, Validation Loss: 0.1568167336305378\nEpoch 78/128, Training Loss: 0.011256203055381775, Validation Loss: 0.15640571076077078\nEpoch 79/128, Training Loss: 0.011275619268417358, Validation Loss: 0.15600006979508432\nEpoch 80/128, Training Loss: 0.011295818723738194, Validation Loss: 0.155599756899519\nEpoch 81/128, Training Loss: 0.011316605843603611, Validation Loss: 0.1552046788490215\nEpoch 82/128, Training Loss: 0.011337997391819954, Validation Loss: 0.15481469381045446\nEpoch 83/128, Training Loss: 0.01135996077209711, Validation Loss: 0.15442977013616277\nEpoch 84/128, Training Loss: 0.011382410302758217, Validation Loss: 0.15404988244777873\nEpoch 85/128, Training Loss: 0.011405473574995995, Validation Loss: 0.15367493455257142\nEpoch 86/128, Training Loss: 0.011428930796682835, Validation Loss: 0.15330491387226117\nEpoch 87/128, Training Loss: 0.011452806182205677, Validation Loss: 0.15293959266227397\nEpoch 88/128, Training Loss: 0.011477128602564335, Validation Loss: 0.1525789823093247\nEpoch 89/128, Training Loss: 0.01150205172598362, Validation Loss: 0.1522230590608246\nEpoch 90/128, Training Loss: 0.01152736134827137, Validation Loss: 0.15187170298237482\nEpoch 91/128, Training Loss: 0.011553069576621056, Validation Loss: 0.1515249170124977\nEpoch 92/128, Training Loss: 0.011579165235161781, Validation Loss: 0.15118256910696065\nEpoch 93/128, Training Loss: 0.011605547741055489, Validation Loss: 0.15084460686720688\nEpoch 94/128, Training Loss: 0.011632390320301056, Validation Loss: 0.15051092407521924\nEpoch 95/128, Training Loss: 0.011659406125545502, Validation Loss: 0.150181468347157\nEpoch 96/128, Training Loss: 0.011686878278851509, Validation Loss: 0.14985620355993542\nEpoch 97/128, Training Loss: 0.011714905500411987, Validation Loss: 0.14953501103527136\nEpoch 98/128, Training Loss: 0.011743108741939068, Validation Loss: 0.14921792587987787\nEpoch 99/128, Training Loss: 0.011771565303206444, Validation Loss: 0.14890477763309484\nEpoch 100/128, Training Loss: 0.01180026400834322, Validation Loss: 0.1485956317676272\nEpoch 101/128, Training Loss: 0.011829249560832977, Validation Loss: 0.14829036803563414\nEpoch 102/128, Training Loss: 0.011858608573675156, Validation Loss: 0.14798891291198188\nEpoch 103/128, Training Loss: 0.011888179928064346, Validation Loss: 0.1476911742157656\nEpoch 104/128, Training Loss: 0.011918027885258198, Validation Loss: 0.14739718618050196\nEpoch 105/128, Training Loss: 0.011948077008128166, Validation Loss: 0.1471068262958943\nEpoch 106/128, Training Loss: 0.011978469789028168, Validation Loss: 0.14682005729654019\nEpoch 107/128, Training Loss: 0.012009156867861748, Validation Loss: 0.14653679932275954\nEpoch 108/128, Training Loss: 0.012039980851113796, Validation Loss: 0.14625702658497747\nEpoch 109/128, Training Loss: 0.012070994824171066, Validation Loss: 0.145980693383357\nEpoch 110/128, Training Loss: 0.012102212756872177, Validation Loss: 0.14570769084833698\nEpoch 111/128, Training Loss: 0.012133536860346794, Validation Loss: 0.14543795447269953\nEpoch 112/128, Training Loss: 0.012165111489593983, Validation Loss: 0.14517159636046975\nEpoch 113/128, Training Loss: 0.012196891941130161, Validation Loss: 0.14490842677247334\nEpoch 114/128, Training Loss: 0.012228680774569511, Validation Loss: 0.14464842494830063\nEpoch 115/128, Training Loss: 0.012260504066944122, Validation Loss: 0.14439156673655063\nEpoch 116/128, Training Loss: 0.012292679399251938, Validation Loss: 0.14413770463025194\nEpoch 117/128, Training Loss: 0.012324923649430275, Validation Loss: 0.14388698049039214\nEpoch 118/128, Training Loss: 0.012357302941381931, Validation Loss: 0.1436392640900453\nEpoch 119/128, Training Loss: 0.012389837764203548, Validation Loss: 0.14339440924824787\nEpoch 120/128, Training Loss: 0.012422428466379642, Validation Loss: 0.14315242790138727\nEpoch 121/128, Training Loss: 0.01245509460568428, Validation Loss: 0.14291331673513066\nEpoch 122/128, Training Loss: 0.0124876843765378, Validation Loss: 0.1426770184111718\nEpoch 123/128, Training Loss: 0.012520410120487213, Validation Loss: 0.14244352688690518\nEpoch 124/128, Training Loss: 0.012553091160953045, Validation Loss: 0.14221276708880312\nEpoch 125/128, Training Loss: 0.012585869058966637, Validation Loss: 0.14198472458021075\nEpoch 126/128, Training Loss: 0.012618696317076683, Validation Loss: 0.14175933752544417\nEpoch 127/128, Training Loss: 0.012651726603507996, Validation Loss: 0.14153652236646658\nEpoch 128/128, Training Loss: 0.012684706598520279, Validation Loss: 0.14131624661014952\nTest Loss: 0.08876131474971771\n","output_type":"stream"},{"name":"stderr","text":"wandb: WARNING No program path found when generating artifact job source for a non-colab notebook run. See https://docs.wandb.ai/guides/launch/create-job\nwandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.164 MB of 0.164 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁</td></tr><tr><td>Test Loss</td><td>▁</td></tr><tr><td>Training Loss</td><td>▃▁▁▂▆█████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>Validation Loss</td><td>█▆▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch_size</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>128</td></tr><tr><td>Test Loss</td><td>0.08876</td></tr><tr><td>Training Loss</td><td>0.01268</td></tr><tr><td>Validation Loss</td><td>0.14132</td></tr><tr><td>batch_size</td><td>1</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">trim-paper-6</strong> at: <a href='https://wandb.ai/ales-2000-09/IEA%20Model/runs/q8jpqwbg' target=\"_blank\">https://wandb.ai/ales-2000-09/IEA%20Model/runs/q8jpqwbg</a><br/>Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240224_152721-q8jpqwbg/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\ny_test_true = y_test.values\ny_test_pred = model(torch.from_numpy(X_test.values).float()).detach().numpy()\n\nr2_test = r2_score(y_test_true, y_test_pred)\nmse_test = mean_squared_error(y_test_true, y_test_pred)\nmae_test = mean_absolute_error(y_test_true, y_test_pred)\n\nprint(f'R2 Score: {r2_test}, Mean Squared Error: {mse_test}, Mean Absolute Error: {mae_test}')","metadata":{"execution":{"iopub.status.busy":"2024-02-24T15:45:24.778107Z","iopub.execute_input":"2024-02-24T15:45:24.778460Z","iopub.status.idle":"2024-02-24T15:45:24.798808Z","shell.execute_reply.started":"2024-02-24T15:45:24.778433Z","shell.execute_reply":"2024-02-24T15:45:24.798172Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"R2 Score: 0.9145332573455509, Mean Squared Error: 0.08876132358211532, Mean Absolute Error: 0.1480066987135397\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}